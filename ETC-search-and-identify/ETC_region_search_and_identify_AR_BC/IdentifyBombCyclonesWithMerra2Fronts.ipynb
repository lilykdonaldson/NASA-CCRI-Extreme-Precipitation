{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c237685-3241-4aa7-b30e-44cc31a5e66c",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ed4da3",
   "metadata": {},
   "source": [
    "This script searches through a year folder of MERRA2 extratropical storm files (found here: https://portal.nccs.nasa.gov/datashare/Obs-ETC/Fronts-ETC/) and identifies ETCs whose path crosses within a specified grid box. Then, pressure drops are identified at the ETCs' center to identify bomb cyclones.\n",
    "\n",
    "Filename convention: MERRA2fronts_YYYYMMDD_UT_Latstorm_Lonstorm_surfacetype_IDofTrack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04679b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import glob\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import os\n",
    "from shapely.geometry import LineString\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b116dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions\n",
    "def is_within_grid(lat, lon, mask_data):\n",
    "    # Find the closest index for the given latitude and longitude\n",
    "    lat_idx = np.abs(mask_data.lat - lat).argmin()\n",
    "    lon_idx = np.abs(mask_data.lon - lon).argmin()\n",
    "    \n",
    "    mask_value = mask_data.mask[lat_idx, lon_idx].values\n",
    "    \n",
    "    if np.isnan(mask_value):\n",
    "        return False\n",
    "    return mask_value == 0 or mask_value == 1\n",
    "\n",
    "def is_line_within_mask(start_lat, start_lon, end_lat, end_lon, mask_data):\n",
    "    line = LineString([(start_lon, start_lat), (end_lon, end_lat)])\n",
    "    for point in line.coords:\n",
    "        if is_within_grid(point[1], point[0], mask_data):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def find_pressure_drop_threshold(latitudes):\n",
    "    \"\"\"Calculate the pressure drop threshold.\"\"\"\n",
    "    return abs(24 * np.sin(np.radians(latitudes)) / np.sin(np.radians(60)))\n",
    "\n",
    "# Function to create line between start and end coordinates\n",
    "def create_line(start_lat, start_lon, end_lat, end_lon, num_points=100):\n",
    "    latitudes = np.linspace(start_lat, end_lat, num_points)\n",
    "    longitudes = np.linspace(start_lon, end_lon, num_points)\n",
    "    return list(zip(latitudes, longitudes))\n",
    "\n",
    "# AR concurrency\n",
    "def check_AR_proximity(lat,lon, ds, timestep):\n",
    "    # Filter the dataset for the northern and western hemispheres\n",
    "    ds_nw = ds.sel(lat=slice(0, 90), lon=slice(-180, 0))\n",
    "\n",
    "    # Box dimensions in kilometers\n",
    "    north_box = 500\n",
    "    south_box = 1500\n",
    "    east_box = 1500\n",
    "    west_box = 500\n",
    "\n",
    "    # Convert kilometers to degrees approximately (assuming 1 degree ~ 111 km)\n",
    "    km_to_deg = 1 / 111\n",
    "    north_lat = lat + north_box * km_to_deg\n",
    "    south_lat = lat - south_box * km_to_deg\n",
    "    east_lon = lon + east_box * km_to_deg\n",
    "    west_lon = lon - west_box * km_to_deg\n",
    "\n",
    "    ds_box = ds_nw.sel(lat=slice(south_lat, north_lat), lon=slice(west_lon, east_lon))\n",
    "    AR_ids = []\n",
    "    for ds_lat in ds_box.lat.values:\n",
    "        for ds_lon in ds_box.lon.values:\n",
    "            kidmap = ds.kidmap.sel(time=timestep, lat=ds_lat, lon=ds_lon, method='nearest').values.item()\n",
    "            if kidmap> 1:\n",
    "                if(kidmap not in AR_ids):\n",
    "                    AR_ids.append(kidmap)\n",
    "    return AR_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f01ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pressure(file_path, lat, lon, x):\n",
    "    \"\"\"Extract the pressure at the closest x indices to the center of the storm\"\"\"\n",
    "    data = xr.open_dataset(file_path, engine='netcdf4')\n",
    "    latitudes = data['latitude'][:, 0]\n",
    "    longitudes = data['longitude'][0, :]\n",
    "    \n",
    "    nearest_lat_idx = int(abs(latitudes - lat).argmin())\n",
    "    nearest_lon_idx = int(abs(longitudes - lon).argmin())\n",
    "    \n",
    "    # Define the range of indices to consider around the nearest point\n",
    "    lat_range = slice(max(0, nearest_lat_idx - x//2), min(len(latitudes), nearest_lat_idx + x//2 + 1))\n",
    "    lon_range = slice(max(0, nearest_lon_idx - x//2), min(len(longitudes), nearest_lon_idx + x//2 + 1))\n",
    "    \n",
    "    pressures = data['MERRA2SLP'].isel(nb_latitude=lat_range, nb_longitude=lon_range).values\n",
    "    #print(f\"Extracted pressures around lat: {lat}, lon: {lon} -> {pressures.flatten()}\")\n",
    "    return pressures.flatten()  # Flatten the array to 1D for easier comparison\n",
    "\n",
    "def findBombCyclones(filenames, mask_data, x, region, AR_dataset_path):\n",
    "    ''' Filter filenames based on coordinates within the grid box and extract unique \n",
    "    storm IDs that pass through the grid box.\n",
    "    '''\n",
    "    unique_storm_ids = set()\n",
    "    for filepath in filenames:\n",
    "        filename = filepath.split(\"/\")[-1]\n",
    "        if filename.startswith(\"MERRA2fronts_\"):\n",
    "            parts = filename.split(\"_\")\n",
    "            lat, lon = float(parts[3]), float(parts[4])\n",
    "            if is_within_grid(lat, lon, mask_data):\n",
    "                unique_storm_ids.add(parts[-1].split('.')[0])\n",
    "\n",
    "    ''' Check each timestep within the storm to see if the pressures around the storm's \n",
    "    center have a drop greater than the threshold based on any previous measurements available.\n",
    "    '''\n",
    "    pressure_drop_events = []\n",
    "    AR_dataset = xr.open_dataset(AR_dataset_path)\n",
    "    for storm_id in unique_storm_ids:\n",
    "        storm_files = [fn for fn in filenames if storm_id in fn.split('/')[-1]]\n",
    "        storm_files.sort()  # Sort files in chronological order\n",
    "\n",
    "        greatest_drop_event = None  # Track the greatest drop for this storm\n",
    "        prev_pressures = []\n",
    "        prev_time = None\n",
    "        start_lat, start_lon = None, None\n",
    "        AR_concurrent_dict = {}\n",
    "        for file_path in storm_files:\n",
    "            try:\n",
    "                parts = file_path.split(\"_\")\n",
    "                date_time = datetime.strptime(parts[1] + parts[2], \"%Y%m%d%H\")\n",
    "                lat, lon = float(parts[3]), float(parts[4])\n",
    "\n",
    "                AR_concurrent_timestep = check_AR_proximity(lat, lon, AR_dataset, date_time)\n",
    "                if AR_concurrent_timestep:\n",
    "                    AR_concurrent_dict[date_time] = AR_concurrent_timestep\n",
    "                \n",
    "                current_pressures = extract_pressure(file_path, lat, lon, x)\n",
    "\n",
    "                if prev_pressures:\n",
    "                    max_prev_pressure = max(prev_pressures)\n",
    "                    min_current_pressure = min(current_pressures)\n",
    "                    max_drop = max_prev_pressure - min_current_pressure\n",
    "                    index_of_prev_pressure = prev_pressures.index(max_prev_pressure)\n",
    "                    \n",
    "                    threshold = find_pressure_drop_threshold(lat)\n",
    "\n",
    "                    #this just checks for the greatest bomb, should we store all bombs that meet the threshold?\n",
    "                    if max_drop >= threshold:\n",
    "                        if greatest_drop_event is None or max_drop > greatest_drop_event['pressure_drop']:\n",
    "                            start_parts = storm_files[0].split(\"_\")\n",
    "                            date_time_start = datetime.strptime(start_parts[1] + start_parts[2], \"%Y%m%d%H\")\n",
    "                            latS, lonS = float(start_parts[3]), float(start_parts[4])\n",
    "                            end_parts = storm_files[-1].split(\"_\")\n",
    "                            date_time_end = datetime.strptime(end_parts[1] + end_parts[2], \"%Y%m%d%H\")\n",
    "                            latE, lonE = float(end_parts[3]), float(end_parts[4])\n",
    "                            \n",
    "                            current_file_index = storm_files.index(file_path)\n",
    "                            bomb_start_file_index = current_file_index-(len(prev_pressures)-index_of_prev_pressure)\n",
    "                            bomb_parts = storm_files[bomb_start_file_index].split(\"_\")\n",
    "                            bomb_date_time_start = datetime.strptime(start_parts[1] + start_parts[2], \"%Y%m%d%H\")\n",
    "                            bomblatS, bomblonS = float(start_parts[3]), float(start_parts[4])\n",
    "                            bomb_start_file_path_name = storm_files[bomb_start_file_index]\n",
    "\n",
    "                            \n",
    "                            greatest_drop_event = {\n",
    "                                'storm_id': storm_id,\n",
    "                                'storm_files': [os.path.basename(file_path) for file_path in storm_files],\n",
    "                                'start_time': date_time_start,\n",
    "                                'end_time': date_time_end,\n",
    "                                'start_lat': latS,\n",
    "                                'start_lon': lonS,\n",
    "                                'end_lat': latE,\n",
    "                                'end_lon': lonE,\n",
    "                                \n",
    "                                'bomb_start_time': bomb_date_time_start,\n",
    "                                'bomb_end_time': date_time,\n",
    "                                'bomb_start_file': os.path.basename(bomb_start_file_path_name),\n",
    "                                'bomb_end_file': os.path.basename(file_path),\n",
    "                                'pressure_drop': max_drop,\n",
    "                                'bomb_start_lat': bomblatS,\n",
    "                                'bomb_start_lon': bomblonS,\n",
    "                                'bomb_end_lat': lat,\n",
    "                                'bomb_end_lon': lon,\n",
    "                                'bomb': True,\n",
    "\n",
    "                                'region': ('northeast' if region == 'east' and lat >= 38 else\n",
    "                                    'southeast' if region == 'east' and lat < 38 else\n",
    "                                    'northwest' if region == 'west' and lat >= 42 else\n",
    "                                    'southwest' if region == 'west' and lat < 42 else None)\n",
    "                            }\n",
    "                # Keep the list to the last 4 pressures + current\n",
    "                prev_pressures.append(min(current_pressures))\n",
    "                if len(prev_pressures) > 4:  # rolling window of the last 4 measurements\n",
    "                    prev_pressures.pop(0)\n",
    "\n",
    "                if prev_time is None:  # Save the start point of the first pressure drop\n",
    "                    start_lat, start_lon = lat, lon\n",
    "\n",
    "                prev_time = date_time\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "                continue  # Ignores corrupted netCDF files\n",
    "\n",
    "        # Add the greatest drop event for this storm, if found, to the list\n",
    "        if greatest_drop_event:\n",
    "            #add AR concurrency dicts\n",
    "            if AR_concurrent_dict:\n",
    "                greatest_drop_event['AR_IDs'] = AR_concurrent_dict\n",
    "                greatest_drop_event['AR_storm_concurrent'] = True\n",
    "                #check if pressure drop point is AR concurrent\n",
    "                greatest_drop_event['AR_bomb_concurrent'] = len(check_AR_proximity(greatest_drop_event['bomb_end_lat'], greatest_drop_event['bomb_end_lon'], AR_dataset, greatest_drop_event['bomb_end_time']))\n",
    "            \n",
    "            else:\n",
    "                greatest_drop_event['AR_IDs'] = None\n",
    "                greatest_drop_event['AR_storm_concurrent'] = False\n",
    "                greatest_drop_event['AR_bomb_concurrent'] = 0\n",
    "                \n",
    "            if is_line_within_mask(greatest_drop_event['bomb_start_lat'], greatest_drop_event['bomb_start_lon'],\n",
    "                                   greatest_drop_event['bomb_end_lat'], greatest_drop_event['bomb_end_lon'], mask_data):\n",
    "                greatest_drop_event['bomb_in_mask'] = True\n",
    "                pressure_drop_events.append(greatest_drop_event)\n",
    "            else:\n",
    "                if is_line_within_mask(greatest_drop_event['start_lat'], greatest_drop_event['start_lon'],\n",
    "                                   greatest_drop_event['end_lat'], greatest_drop_event['end_lon'], mask_data):\n",
    "                    greatest_drop_event['bomb_in_mask'] = False\n",
    "                    pressure_drop_events.append(greatest_drop_event)\n",
    "        else:\n",
    "            start_parts = storm_files[0].split(\"_\")\n",
    "            date_time_start = datetime.strptime(start_parts[1] + start_parts[2], \"%Y%m%d%H\")\n",
    "            latS, lonS = float(start_parts[3]), float(start_parts[4])\n",
    "            end_parts = storm_files[-1].split(\"_\")\n",
    "            date_time_end = datetime.strptime(end_parts[1] + end_parts[2], \"%Y%m%d%H\")\n",
    "            latE, lonE = float(end_parts[3]), float(end_parts[4])\n",
    "            #check if storm path is in mask\n",
    "            if is_line_within_mask(latS, lonS, latE, lonE, mask_data):\n",
    "                non_bomb = {\n",
    "                                    'storm_id': storm_id,\n",
    "                                    'start_time': date_time_start,\n",
    "                                    'end_time': date_time_end,\n",
    "                                    'start_lat': latS,\n",
    "                                    'start_lon': lonS,\n",
    "                                    'end_lat': latE,\n",
    "                                    'end_lon': lonE,\n",
    "                                    'storm_files': [os.path.basename(file_path) for file_path in storm_files],\n",
    "                    \n",
    "                                    'bomb': False,\n",
    "                                    'bomb_in_mask': None,\n",
    "                                    'bomb_start_time': None,\n",
    "                                    'bomb_end_time': None,\n",
    "                                    'pressure_drop': None,\n",
    "                                    'bomb_start_lat': None,\n",
    "                                    'bomb_start_lon': None,\n",
    "                                    'bomb_end_lat': None,\n",
    "                                    'bomb_end_lon': None,\n",
    "                                    'AR_bomb_concurrent': None,\n",
    "\n",
    "                                    #maybe change in future to check quad for storm in region mask\n",
    "                                    'region': ('northeast' if region == 'east' and latE >= 38 else\n",
    "                                    'southeast' if region == 'east' and latE < 38 else\n",
    "                                    'northwest' if region == 'west' and latE >= 42 else\n",
    "                                    'southwest' if region == 'west' and latE < 42 else None)\n",
    "                                }\n",
    "                if AR_concurrent_dict:\n",
    "                    non_bomb['AR_IDs'] = AR_concurrent_dict\n",
    "                    non_bomb['AR_storm_concurrent'] = True\n",
    "                else:\n",
    "                    non_bomb['AR_IDs'] = None\n",
    "                    non_bomb['AR_storm_concurrent'] = False\n",
    "                pressure_drop_events.append(non_bomb)\n",
    "            \n",
    "            \n",
    "\n",
    "    return pressure_drop_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b576bf5-8772-4898-990c-8144e8da4f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lat_lon_from_filename(filename):\n",
    "    parts = filename.split('_')\n",
    "    try:\n",
    "        lat_str = parts[3]\n",
    "        lon_str = parts[4]\n",
    "        lat = float(lat_str)\n",
    "        lon = float(lon_str)\n",
    "        return lat, lon\n",
    "    except (IndexError, ValueError) as e:\n",
    "        raise ValueError(f\"Latitude and longitude not found in filename: {filename}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e53798a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "region = 'west'\n",
    "mask_file = f\"/Users/lilydonaldson/Downloads/examples/util/masks/{region}coast_mask_landocean.nc\"\n",
    "mask_data = xr.open_dataset(mask_file)\n",
    "\n",
    "year_start = 2010\n",
    "year_end = 2010\n",
    "num_gridboxes_to_check = 9 #x by x grid around center point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "715e538b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing year: 2010\n",
      "# of files found for 2010\n",
      "58934\n",
      "Saved 34 events for year 2010 to /Users/lilydonaldson/Downloads/examples/data/west_ETCs_2010.pkl\n",
      "ETCs identified within the Year 2010 for west\n",
      "Number of ETCs Identified 2010:\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "for year in range(year_start, year_end + 1):\n",
    "    print(f\"Processing year: {year}\")\n",
    "    folder_path = f\"/Volumes/SSK Drive /merra2fronts/merra2fronts{year}/*/*\"\n",
    "    # Grab all of the NetCDF storm files for an entire year\n",
    "    filenames = glob.glob(folder_path, recursive=True)\n",
    "    print(f\"# of files found for {year}\")\n",
    "    print(len(filenames))\n",
    "\n",
    "    AR_dataset = f\"/Users/lilydonaldson/Downloads/examples/data/AR_out/GISS_ARout5_WISO_20th_MERRA2_ANL_{year}.nc\"\n",
    "    \n",
    "    pressure_drop_events = findBombCyclones(filenames, mask_data, num_gridboxes_to_check,region, AR_dataset)\n",
    "    \n",
    "    output_dir = \"/Users/lilydonaldson/Downloads/examples/data/\"\n",
    "    f_path = folder_path.replace(\"/*/*\",\"\")\n",
    "    output_path = os.path.join(output_dir, f'{region}_ETCs_{year}.pkl')\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(pressure_drop_events, f)\n",
    "    print(f\"Saved {len(pressure_drop_events)} events for year {year} to {output_path}\")\n",
    "    \n",
    "    print(f\"ETCs identified within the Year {year} for {region}\")\n",
    "    print(f\"Number of ETCs Identified {year}:\")\n",
    "    print(len(pressure_drop_events))\n",
    "    # # Print the pressure drop events\n",
    "    # for event in pressure_drop_events:\n",
    "    #     rounded_drop = f\"{event['pressure_drop']:.2f}\"\n",
    "    #     print(f\"Storm ID: {event['storm_id']}, Start Time: {event['start_time'].strftime('%Y-%m-%d %H:%M:%S')}, End Time: {event['end_time'].strftime('%Y-%m-%d %H:%M:%S')}, \\n     Pressure Drop: {rounded_drop} hPa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad21b002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "{'storm_id': '20100608120510023850', 'start_time': datetime.datetime(2010, 6, 8, 12, 0), 'end_time': datetime.datetime(2010, 6, 11, 18, 0), 'start_lat': 39.0, 'start_lon': -121.5, 'end_lat': 57.0, 'end_lon': -123.0, 'storm_files': ['MERRA2fronts_20100608_12_39.00_-121.50_land_20100608120510023850.ncdf', 'MERRA2fronts_20100608_18_41.45_-121.79_land_20100608120510023850.ncdf', 'MERRA2fronts_20100609_00_44.77_-119.80_land_20100608120510023850.ncdf', 'MERRA2fronts_20100609_06_47.25_-117.90_land_20100608120510023850.ncdf', 'MERRA2fronts_20100609_12_47.62_-115.63_land_20100608120510023850.ncdf', 'MERRA2fronts_20100609_18_50.78_-119.25_land_20100608120510023850.ncdf', 'MERRA2fronts_20100610_00_52.34_-121.29_land_20100608120510023850.ncdf', 'MERRA2fronts_20100610_06_52.17_-121.38_land_20100608120510023850.ncdf', 'MERRA2fronts_20100610_12_51.90_-121.79_land_20100608120510023850.ncdf', 'MERRA2fronts_20100610_18_52.68_-121.26_land_20100608120510023850.ncdf', 'MERRA2fronts_20100611_00_54.67_-124.04_land_20100608120510023850.ncdf', 'MERRA2fronts_20100611_06_55.29_-124.56_land_20100608120510023850.ncdf', 'MERRA2fronts_20100611_12_56.25_-123.11_land_20100608120510023850.ncdf', 'MERRA2fronts_20100611_18_57.00_-123.00_land_20100608120510023850.ncdf'], 'bomb': False, 'bomb_in_mask': None, 'bomb_start_time': None, 'bomb_end_time': None, 'pressure_drop': None, 'bomb_start_lat': None, 'bomb_start_lon': None, 'bomb_end_lat': None, 'bomb_end_lon': None, 'AR_bomb_concurrent': None, 'region': 'northwest', 'AR_IDs': {datetime.datetime(2010, 6, 8, 12, 0): [1167.0], datetime.datetime(2010, 6, 8, 18, 0): [1167.0], datetime.datetime(2010, 6, 9, 0, 0): [1167.0], datetime.datetime(2010, 6, 9, 6, 0): [1167.0], datetime.datetime(2010, 6, 9, 12, 0): [1188.0], datetime.datetime(2010, 6, 10, 12, 0): [1198.0], datetime.datetime(2010, 6, 11, 6, 0): [1167.0], datetime.datetime(2010, 6, 11, 12, 0): [1167.0], datetime.datetime(2010, 6, 11, 18, 0): [1167.0]}, 'AR_storm_concurrent': True}\n"
     ]
    }
   ],
   "source": [
    "with open(f\"/Users/lilydonaldson/Downloads/examples/data/west_ETCs_2010.pkl\", 'rb') as f:\n",
    "    BC = pickle.load(f)\n",
    "print(len(BC))\n",
    "print(BC[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748831cf-6303-492f-9c33-5ce657c6130c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
