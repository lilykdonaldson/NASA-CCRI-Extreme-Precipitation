{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c237685-3241-4aa7-b30e-44cc31a5e66c",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04679b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import glob\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "import csv\n",
    "import os\n",
    "from shapely.geometry import LineString\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b116dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions\n",
    "def is_within_grid(lat, lon, mask_data):\n",
    "    # Find the closest index for the given latitude and longitude\n",
    "    lat_idx = np.abs(mask_data.lat - lat).argmin()\n",
    "    lon_idx = np.abs(mask_data.lon - lon).argmin()\n",
    "    \n",
    "    mask_value = mask_data.mask[lat_idx, lon_idx].values\n",
    "    if np.isnan(mask_value):\n",
    "        return False\n",
    "    return mask_value == 0 or mask_value == 1\n",
    "\n",
    "def is_line_within_mask(start_lat, start_lon, end_lat, end_lon, mask_data, num_points=50):\n",
    "    lats = np.linspace(start_lat, end_lat, num_points)\n",
    "    lons = np.linspace(start_lon, end_lon, num_points)\n",
    "    save = False\n",
    "    for lat, lon in zip(lats, lons):\n",
    "        if is_within_grid(lat, lon, mask_data):\n",
    "            save = True\n",
    "    return save\n",
    "\n",
    "def find_pressure_drop_threshold(latitudes):\n",
    "    \"\"\"Calculate the pressure drop threshold.\"\"\"\n",
    "    return abs(24 * np.sin(np.radians(latitudes)) / np.sin(np.radians(60)))\n",
    "\n",
    "# Function to create line between start and end coordinates\n",
    "def create_line(start_lat, start_lon, end_lat, end_lon, num_points=100):\n",
    "    latitudes = np.linspace(start_lat, end_lat, num_points)\n",
    "    longitudes = np.linspace(start_lon, end_lon, num_points)\n",
    "    return list(zip(latitudes, longitudes))\n",
    "\n",
    "# AR concurrency\n",
    "def check_AR_proximity(lat, lon, ds, timestep):\n",
    "    # Box dimensions in kilometers\n",
    "    north_box = 500\n",
    "    south_box = 1500\n",
    "    east_box = 1500\n",
    "    west_box = 500\n",
    "\n",
    "    # Convert kilometers to degrees approximately (assuming 1 degree ~ 111 km)\n",
    "    km_to_deg = 1 / 111\n",
    "    north_lat = lat + north_box * km_to_deg\n",
    "    south_lat = lat - south_box * km_to_deg\n",
    "    east_lon = lon + east_box * km_to_deg\n",
    "    west_lon = lon - west_box * km_to_deg\n",
    "\n",
    "    # Find indices for the bounding box\n",
    "    lat_inds = np.where((ds.lat >= south_lat) & (ds.lat <= north_lat))[0]\n",
    "    lon_inds = np.where((ds.lon >= west_lon) & (ds.lon <= east_lon))[0]\n",
    "\n",
    "    # Subset the dataset using indices\n",
    "    ds_box = ds.isel(lat=lat_inds, lon=lon_inds)\n",
    "\n",
    "    # Check if lat/lon is in a gridbox where kidmap is >1\n",
    "    AR_ids = []\n",
    "    kidmap = ds['kidmap'].sel(time=timestep, method='nearest')\n",
    "\n",
    "    for lat_ind in lat_inds:\n",
    "        for lon_ind in lon_inds:\n",
    "            kidmap_value = kidmap.isel(lat=lat_ind, lon=lon_ind).values.item()\n",
    "            if kidmap_value > 1:\n",
    "                if kidmap_value not in AR_ids:\n",
    "                    AR_ids.append(kidmap_value)\n",
    "                    return AR_ids\n",
    "    return AR_ids \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f01ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pressure(data, lat, lon, x):\n",
    "    \"\"\"Extract the pressure at the closest x indices to the center of the storm\"\"\"\n",
    "    latitudes = data['Latitude'].unique()\n",
    "    longitudes = data['Longitude'].unique()\n",
    "    \n",
    "    nearest_lat_idx = int(np.abs(latitudes - lat).argmin())\n",
    "    nearest_lon_idx = int(np.abs(longitudes - lon).argmin())\n",
    "    \n",
    "    # Define the range of indices to consider around the nearest point\n",
    "    lat_range = latitudes[max(0, nearest_lat_idx - x//2): min(len(latitudes), nearest_lat_idx + x//2 + 1)]\n",
    "    lon_range = longitudes[max(0, nearest_lon_idx - x//2): min(len(longitudes), nearest_lon_idx + x//2 + 1)]\n",
    "    \n",
    "    pressures = data[(data['Latitude'].isin(lat_range)) & (data['Longitude'].isin(lon_range))]['Sea_level_pressure'].values\n",
    "    return pressures.flatten()  # Flatten the array to 1D for easier comparison\n",
    "\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculate the geodesic distance between two lat/lon points\"\"\"\n",
    "    return geodesic((lat1, lon1), (lat2, lon2)).kilometers\n",
    "\n",
    "def findBombCyclones(data, mask_data, x, region, AR_dataset):\n",
    "    #print(AR_dataset.head())\n",
    "    ''' Filter data based on coordinates within the grid box and extract unique \n",
    "    storm IDs that pass through the grid box.\n",
    "    '''\n",
    "    unique_storm_ids = data['USI'].unique()\n",
    "    complete = len(unique_storm_ids)\n",
    "    print(f\"# ids: {len(unique_storm_ids)}\")\n",
    "    \n",
    "    ''' Check each timestep within the storm to see if the pressures around the storm's \n",
    "    center have a drop greater than the threshold based on any previous measurements available.\n",
    "    '''\n",
    "    with tqdm(total=complete, desc=\"Processing Storms\", unit=\"storm\") as pbar:\n",
    "        pressure_drop_events = []\n",
    "        for storm_id in unique_storm_ids:\n",
    "            storm_data = data[data['USI'] == storm_id]\n",
    "            storm_data = storm_data.sort_values(by=['Year', 'Month', 'Day', 'Hour'])  # Sort data in chronological order\n",
    "    \n",
    "            greatest_drop_event = None  # Track the greatest drop for this storm\n",
    "            prev_pressures = []\n",
    "            prev_time = None\n",
    "            start_lat, start_lon = None, None\n",
    "            AR_concurrent_dict = {}\n",
    "            distances = []\n",
    "            prev_lat, prev_lon = None, None\n",
    "            for _, row in storm_data.iterrows():\n",
    "                try:\n",
    "                    date_time = datetime(row['Year'], row['Month'], row['Day'], row['Hour'])\n",
    "                    lat, lon = row['Latitude'], row['Longitude']\n",
    "    \n",
    "                    if prev_lat is not None and prev_lon is not None:\n",
    "                        distances.append(calculate_distance(prev_lat, prev_lon, lat, lon))\n",
    "                    prev_lat, prev_lon = lat, lon\n",
    "                    # AR_concurrent_timestep = check_AR_proximity(lat, lon, AR_dataset, date_time)\n",
    "                    # if AR_concurrent_timestep:\n",
    "                    #     AR_concurrent_dict[date_time] = AR_concurrent_timestep\n",
    "                    \n",
    "                    current_pressures = extract_pressure(storm_data, lat, lon, x)\n",
    "    \n",
    "                    if prev_pressures:\n",
    "                        max_prev_pressure = max(prev_pressures)\n",
    "                        min_current_pressure = min(current_pressures)\n",
    "                        max_drop = max_prev_pressure - min_current_pressure\n",
    "                        index_of_prev_pressure = prev_pressures.index(max_prev_pressure)\n",
    "                        \n",
    "                        threshold = find_pressure_drop_threshold(lat)\n",
    "    \n",
    "                        # This just checks for the greatest bomb, should we store all bombs that meet the threshold?\n",
    "                        if max_drop >= threshold:\n",
    "                            \n",
    "                            if greatest_drop_event is None or max_drop > greatest_drop_event['pressure_drop']:\n",
    "                                start_row = storm_data.iloc[0]\n",
    "                                end_row = storm_data.iloc[-1]\n",
    "                                date_time_start = datetime(start_row['Year'], start_row['Month'], start_row['Day'], start_row['Hour'])\n",
    "                                latS, lonS = start_row['Latitude'], start_row['Longitude']\n",
    "                                date_time_end = datetime(end_row['Year'], end_row['Month'], end_row['Day'], end_row['Hour'])\n",
    "                                latE, lonE = end_row['Latitude'], end_row['Longitude']\n",
    "                                \n",
    "                                current_row_index = storm_data.index.get_loc(row.name)\n",
    "                                bomb_start_row_index = current_row_index - (len(prev_pressures) - index_of_prev_pressure)\n",
    "                                bomb_start_row = storm_data.iloc[bomb_start_row_index]\n",
    "                                bomb_date_time_start = datetime(bomb_start_row['Year'], bomb_start_row['Month'], bomb_start_row['Day'], bomb_start_row['Hour'])\n",
    "                                bomblatS, bomblonS = bomb_start_row['Latitude'], bomb_start_row['Longitude']\n",
    "                                \n",
    "                                greatest_drop_event = {\n",
    "                                    'storm_id': storm_id,\n",
    "                                    'start_time': date_time_start,\n",
    "                                    'end_time': date_time_end,\n",
    "                                    'start_lat': latS,\n",
    "                                    'start_lon': lonS,\n",
    "                                    'end_lat': latE,\n",
    "                                    'end_lon': lonE,\n",
    "                                    \n",
    "                                    'bomb_start_time': bomb_date_time_start,\n",
    "                                    'bomb_end_time': date_time,\n",
    "                                    'pressure_drop': max_drop,\n",
    "                                    'bomb_start_lat': bomblatS,\n",
    "                                    'bomb_start_lon': bomblonS,\n",
    "                                    'bomb_end_lat': lat,\n",
    "                                    'bomb_end_lon': lon,\n",
    "                                    'bomb': True,\n",
    "                                    'distance_between_timesteps': distances,\n",
    "    \n",
    "                                    'region': ('northeast' if region == 'east' and lat >= 38 else\n",
    "                                        'southeast' if region == 'east' and lat < 38 else\n",
    "                                        'northwest' if region == 'west' and lat >= 42 else\n",
    "                                        'southwest' if region == 'west' and lat < 42 else None)\n",
    "                                }\n",
    "                    # Keep the list to the last 4 pressures + current\n",
    "                    prev_pressures.append(min(current_pressures))\n",
    "                    if len(prev_pressures) > 4:  # rolling window of the last 4 measurements\n",
    "                        prev_pressures.pop(0)\n",
    "    \n",
    "                    if prev_time is None:  # Save the start point of the first pressure drop\n",
    "                        start_lat, start_lon = lat, lon\n",
    "    \n",
    "                    prev_time = date_time\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing row: {e}\")\n",
    "                    continue  # Ignores corrupted netCDF files\n",
    "    \n",
    "            # Add the greatest drop event for this storm, if found, to the list\n",
    "            if greatest_drop_event:\n",
    "                pbar.update(1)\n",
    "                #add AR concurrency dicts\n",
    "                if AR_concurrent_dict:\n",
    "                    greatest_drop_event['AR_IDs'] = AR_concurrent_dict\n",
    "                    greatest_drop_event['AR_storm_concurrent'] = True\n",
    "                    #check if pressure drop point is AR concurrent\n",
    "                    greatest_drop_event['AR_bomb_concurrent'] = len(check_AR_proximity(greatest_drop_event['bomb_end_lat'], greatest_drop_event['bomb_end_lon'], AR_dataset, greatest_drop_event['bomb_end_time']))\n",
    "                \n",
    "                else:\n",
    "                    greatest_drop_event['AR_IDs'] = None\n",
    "                    greatest_drop_event['AR_storm_concurrent'] = False\n",
    "                    greatest_drop_event['AR_bomb_concurrent'] = 0\n",
    "                    \n",
    "                if is_line_within_mask(greatest_drop_event['bomb_start_lat'], greatest_drop_event['bomb_start_lon'],\n",
    "                                       greatest_drop_event['bomb_end_lat'], greatest_drop_event['bomb_end_lon'], mask_data):\n",
    "                    greatest_drop_event['bomb_in_mask'] = True\n",
    "                    pressure_drop_events.append(greatest_drop_event)\n",
    "                else:\n",
    "                    if is_line_within_mask(greatest_drop_event['start_lat'], greatest_drop_event['start_lon'],\n",
    "                                       greatest_drop_event['end_lat'], greatest_drop_event['end_lon'], mask_data):\n",
    "                        greatest_drop_event['bomb_in_mask'] = False\n",
    "                        pressure_drop_events.append(greatest_drop_event)\n",
    "            else:\n",
    "                pbar.update(1)\n",
    "                \n",
    "                start_row = storm_data.iloc[0]\n",
    "                end_row = storm_data.iloc[-1]\n",
    "                date_time_start = datetime(start_row['Year'], start_row['Month'], start_row['Day'], start_row['Hour'])\n",
    "                latS, lonS = start_row['Latitude'], start_row['Longitude']\n",
    "                date_time_end = datetime(end_row['Year'], end_row['Month'], end_row['Day'], end_row['Hour'])\n",
    "                latE, lonE = end_row['Latitude'], end_row['Longitude']\n",
    "                #check if storm path is in mask\n",
    "                if is_line_within_mask(latS, lonS, latE, lonE, mask_data):\n",
    "                    non_bomb = {\n",
    "                                        'storm_id': storm_id,\n",
    "                                        'start_time': date_time_start,\n",
    "                                        'end_time': date_time_end,\n",
    "                                        'start_lat': latS,\n",
    "                                        'start_lon': lonS,\n",
    "                                        'end_lat': latE,\n",
    "                                        'end_lon': lonE,\n",
    "                        \n",
    "                                        'bomb': False,\n",
    "                                        'bomb_in_mask': None,\n",
    "                                        'bomb_start_time': None,\n",
    "                                        'bomb_end_time': None,\n",
    "                                        'pressure_drop': None,\n",
    "                                        'bomb_start_lat': None,\n",
    "                                        'bomb_start_lon': None,\n",
    "                                        'bomb_end_lat': None,\n",
    "                                        'bomb_end_lon': None,\n",
    "                                        'AR_bomb_concurrent': None,\n",
    "                                        'distance_between_timesteps': distances,\n",
    "    \n",
    "                                        #maybe change in future to check quad for storm in region mask\n",
    "                                        'region': ('northeast' if region == 'east' and latE >= 38 else\n",
    "                                        'southeast' if region == 'east' and latE < 38 else\n",
    "                                        'northwest' if region == 'west' and latE >= 42 else\n",
    "                                        'southwest' if region == 'west' and latE < 42 else None)\n",
    "                                    }\n",
    "                    if AR_concurrent_dict:\n",
    "                        non_bomb['AR_IDs'] = AR_concurrent_dict\n",
    "                        non_bomb['AR_storm_concurrent'] = True\n",
    "                    else:\n",
    "                        non_bomb['AR_IDs'] = None\n",
    "                        non_bomb['AR_storm_concurrent'] = False\n",
    "                    pressure_drop_events.append(non_bomb)\n",
    "            \n",
    "            \n",
    "\n",
    "    return pressure_drop_events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e53798a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "region = 'west'\n",
    "mask_file = f\"/Users/lilydonaldson/Downloads/examples/util/masks/{region}coast_mask_landocean.nc\"\n",
    "mask_data = xr.open_dataset(mask_file)\n",
    "\n",
    "year_start = 2020\n",
    "year_end = 2020\n",
    "num_gridboxes_to_check = 9 #x by x grid around center point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "715e538b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3x/6dh1l7m535n5hvm8c7tjlk_00000gn/T/ipykernel_26708/76826603.py:9: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  data = pd.read_csv('/Users/lilydonaldson/Downloads/out_era5_output_2018_2020.txt', delim_whitespace=True, names=columns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2020\n",
      "# ids: 304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Storms: 100%|███████████████████| 304/304 [00:06<00:00, 46.75storm/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 0 events for year 2020 to /Users/lilydonaldson/Downloads/examples/data/merra2fronts/identified_bomb_cyclones/updatedJuly22/westCoast/ERA5_ERA5ar_west_ETCs_2020.pkl\n",
      "ETCs identified within the Year 2020 for west\n",
      "Number of ETCs Identified 2020:\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the columns and their positions in the text file\n",
    "columns = [\n",
    "    'Year', 'Month', 'Day', 'Hour', 'Unused1', 'lat_proxy', 'lon_proxy', 'Unused2',\n",
    "    'Sea_level_pressure', 'Unused3', 'Unused4', 'Unused5', 'Unused6',\n",
    "    'Unused7', 'CSI', 'USI'\n",
    "]\n",
    "\n",
    "# Read the text file into a DataFrame\n",
    "data = pd.read_csv('/Users/lilydonaldson/Downloads/out_era5_output_2018_2020.txt', delim_whitespace=True, names=columns)\n",
    "\n",
    "# Convert proxies to actual values\n",
    "data['Latitude'] = 90 - data['lat_proxy'] / 100\n",
    "data['Longitude'] = data['lon_proxy'] / 100\n",
    "data['Longitude'] = data['Longitude'].apply(lambda x: x - 360 if x > 90 else x)\n",
    "data['Sea_level_pressure'] = data['Sea_level_pressure'] / 1000\n",
    "\n",
    "AR_dataset_path = f\"/Volumes/SSK Drive /ERA5_ARs/globalARcatalog_ERA5_1940-2023_v4.0.nc\"\n",
    "AR_dataset = xr.open_dataset(AR_dataset_path)\n",
    "for year in range(year_start, year_end + 1):\n",
    "    print(f\"Processing {year}\")\n",
    "    #just grab year of data from data[]\n",
    "    current_year_data = data[data['Year'] == year]\n",
    "    \n",
    "    \n",
    "    pressure_drop_events = findBombCyclones(current_year_data, mask_data, num_gridboxes_to_check,region, AR_dataset)\n",
    "    \n",
    "    output_dir = f\"/Users/lilydonaldson/Downloads/examples/data/merra2fronts/identified_bomb_cyclones/updatedJuly22/{region}Coast/\"\n",
    "    output_path = os.path.join(output_dir, f'ERA5_ERA5ar_{region}_ETCs_{year}.pkl')\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(pressure_drop_events, f)\n",
    "    print(f\"Saved {len(pressure_drop_events)} events for year {year} to {output_path}\")\n",
    "    \n",
    "    print(f\"ETCs identified within the Year {year} for {region}\")\n",
    "    print(f\"Number of ETCs Identified {year}:\")\n",
    "    print(len(pressure_drop_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad21b002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'storm_id': '20000112120525025800', 'start_time': datetime.datetime(2000, 1, 12, 12, 0), 'end_time': datetime.datetime(2000, 1, 15, 0, 0), 'start_lat': 37.7, 'start_lon': -101.47000000000003, 'end_lat': 54.63, 'end_lon': -49.639999999999986, 'bomb_start_time': datetime.datetime(2000, 1, 12, 18, 0), 'bomb_end_time': datetime.datetime(2000, 1, 13, 18, 0), 'pressure_drop': 20.418000000000006, 'bomb_start_lat': 38.16, 'bomb_start_lon': -94.55000000000001, 'bomb_end_lat': 40.02, 'bomb_end_lon': -70.48000000000002, 'bomb': True, 'distance_between_timesteps': [610.3727054133809, 444.4756297167836, 479.3758784505471, 495.9863614167866, 679.539915905261, 533.0143957248911, 636.3894889080922, 354.26365543009297, 519.1016151923093, 502.85596987288307], 'region': 'northeast', 'AR_IDs': None, 'AR_storm_concurrent': False, 'AR_bomb_concurrent': 0, 'bomb_in_mask': True}\n",
      "Number of AR_storm_concurrent events: 0\n"
     ]
    }
   ],
   "source": [
    "# Define the path to your .pkl file\n",
    "file_path = \"/Users/lilydonaldson/Downloads/examples/data/merra2fronts/identified_bomb_cyclones/updatedJuly22/eastCoast/ERA5_ERA5ar_east_ETCs_2000.pkl\"\n",
    "\n",
    "# Load the .pkl file\n",
    "with open(file_path, 'rb') as f:\n",
    "    events = pickle.load(f)\n",
    "print(events[0])\n",
    "# Check the number of events that are AR_storm_concurrent\n",
    "ar_storm_concurrent_count = sum(event['AR_storm_concurrent'] for event in events if 'AR_storm_concurrent' in event)\n",
    "\n",
    "print(f\"Number of AR_storm_concurrent events: {ar_storm_concurrent_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
